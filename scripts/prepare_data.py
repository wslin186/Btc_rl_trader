# -*- coding: utf-8 -*-
"""
prepare_data.py
--------------------------------------------------
è¯»å– Binance .zip / .csvï¼ˆæ— è¡¨å¤´ï¼‰ï¼Œåˆå¹¶ BTCUSDT 1h K çº¿ï¼š
1. é€’å½’å¤„ç† monthly + daily ç›®å½•
2. è¿‡æ»¤å¼‚å¸¸æ—¶é—´æˆ³ã€NaN/Inf
3. æ·»åŠ  RSI / MACD / EMA
4. ä¿å­˜åˆ° data/processed/btc.parquet
"""

from __future__ import annotations

from itertools import chain
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm

from utils.feature_engineering import add_indicators
from utils.logger import get_logger

logger = get_logger("prepare_data")

# ----------------------------------------------------------------------
# è·¯å¾„ & é…ç½®
# ----------------------------------------------------------------------
RAW_DIR = Path("data/raw/data/spot")          # ä¸‹è½½è„šæœ¬é»˜è®¤å­˜æ”¾æ ¹ç›®å½•
OUT_PATH = Path("data/processed/btc.parquet")
SYMBOLS = ["BTCUSDT"]                         # å¤šå¸ç§å¯æ‰©å±•æ­¤åˆ—è¡¨

COLS = [
    "open_time", "open", "high", "low", "close", "volume",
    "close_time", "quote_asset_volume", "number_of_trades",
    "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore",
]
DTYPES = {c: "Int64" if c.endswith("_time") else "float64" for c in COLS}
NEEDED = {"open_time", "open", "high", "low", "close", "volume"}

# åˆç†æ—¶é—´æˆ³èŒƒå›´ï¼ˆæ¯«ç§’ï¼‰
T_MIN, T_MAX = 1_483_228_800_000, 4_107_926_400_000  # 2017â€‘01â€‘01 ~ 2100â€‘01â€‘01


# ----------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# ----------------------------------------------------------------------
def read_kline(path: Path) -> pd.DataFrame:
    """è¯»å–å•ä¸ª ZIP/CSVï¼Œæ— è¡¨å¤´ -> names=COLS"""
    return pd.read_csv(
        path,
        header=None,
        names=COLS,
        compression="zip" if path.suffix == ".zip" else None,
        dtype=DTYPES,
        na_values=["", "nan"],
    )


# ----------------------------------------------------------------------
# ä¸»æµç¨‹
# ----------------------------------------------------------------------
def main() -> None:
    # 1. æ”¶é›†æ–‡ä»¶ (.zip + .csv)
    files = [
        f for f in chain(RAW_DIR.rglob("*.zip"), RAW_DIR.rglob("*.csv"))
        if any(sym in f.name for sym in SYMBOLS)
    ]
    logger.info(f"DEBUG | åŒ¹é…åˆ°æ–‡ä»¶æ•°: {len(files)}")
    if not files:
        logger.error(f"âŒ æœªæ‰¾åˆ° {SYMBOLS} çš„ zip/csv")
        return

    # 2. è¯»å–å¹¶åŸºæœ¬æ ¡éªŒ
    dfs: list[pd.DataFrame] = []
    total_rows, skipped = 0, 0

    for f in tqdm(files, desc="Reading files", ncols=80):
        try:
            df = read_kline(f)
        except Exception as e:
            logger.warning(f"âš ï¸  è¯»å–å¤±è´¥ {f.name}: {e}")
            skipped += 1
            continue

        if df.empty or not NEEDED.issubset(df.columns):
            logger.warning(f"âš ï¸  è·³è¿‡ç©ºè¡¨/åˆ—ç¼ºå¤±: {f.name}")
            skipped += 1
            continue

        dfs.append(df)
        total_rows += len(df)
        if total_rows % 1_000_000 < len(df):
            logger.info(f"ğŸ’¾ å·²ç´¯ç§¯ {total_rows:,} è¡Œâ€¦")

    if not dfs:
        logger.error("âŒ æ‰€æœ‰æ–‡ä»¶è¢«è·³è¿‡ï¼Œæ— æ³•ç»§ç»­")
        return

    # 3. åˆå¹¶
    df = pd.concat(dfs, ignore_index=True)
    logger.info(
        f"ğŸ“Š æœ‰æ•ˆæ–‡ä»¶ {len(dfs)}/{len(files)}ï¼Œæ‹¼æ¥å {len(df):,} è¡Œ"
    )

    # 4. è¿‡æ»¤å¼‚å¸¸æ—¶é—´æˆ³å¹¶æ•´ç†åˆ—
    df = df[df["open_time"].between(T_MIN, T_MAX)]
    df = df[["open_time", "open", "high", "low", "close", "volume"]]
    df.columns = ["timestamp", "open", "high", "low", "close", "volume"]
    df["timestamp"] = pd.to_datetime(
        df["timestamp"], unit="ms", errors="coerce"
    )
    df = df.dropna(subset=["timestamp"])

    # 5. æŠ€æœ¯æŒ‡æ ‡ + æ¸…æ´— (å‡½æ•°å·²ç¡®ä¿æ—  NaN/Inf)
    df = add_indicators(df)

    # 6. ä¿å­˜
    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(OUT_PATH, index=False)
    logger.info(f"âœ… Saved processed data to {OUT_PATH}")


# ----------------------------------------------------------------------
if __name__ == "__main__":
    main()
