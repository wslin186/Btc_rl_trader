# -*- coding: utf-8 -*-
"""
prepare_data.py
--------------------------------------------------
è¯»å– Binance .zip / .csvï¼ˆæ— è¡¨å¤´ï¼‰ï¼Œåˆå¹¶ BTCUSDT 1h K çº¿ï¼š
1. é€’å½’å¤„ç† monthly + daily ç›®å½•
2. è¿‡æ»¤å¼‚å¸¸æ—¶é—´æˆ³ã€NaN/Inf
3. æ·»åŠ å¤šç§æŠ€æœ¯æŒ‡æ ‡
4. ç»Ÿè®¡æ•°æ®è´¨é‡ä¸åˆ†å¸ƒ
5. ä¿å­˜åˆ° data/processed/btc.parquet
"""

from __future__ import annotations

import sys
from pathlib import Path
ROOT = Path(__file__).resolve().parents[1]  # è·å–é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, str(ROOT))  # æ·»åŠ åˆ°Pythonè·¯å¾„

import os
import time
from itertools import chain
import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt

from utils.feature_engineering import add_indicators, normalize_features
from utils.logger import get_logger

logger = get_logger("prepare_data")

# ----------------------------------------------------------------------
# è·¯å¾„ & é…ç½®
# ----------------------------------------------------------------------
RAW_DIR = Path("data/raw/data/spot")          # ä¸‹è½½è„šæœ¬é»˜è®¤å­˜æ”¾æ ¹ç›®å½•
OUT_PATH = Path("data/processed/btc.parquet")
STATS_DIR = Path("data/stats")                # æ•°æ®ç»Ÿè®¡ç›®å½•
SYMBOLS = ["BTCUSDT"]                         # å¤šå¸ç§å¯æ‰©å±•æ­¤åˆ—è¡¨

COLS = [
    "open_time", "open", "high", "low", "close", "volume",
    "close_time", "quote_asset_volume", "number_of_trades",
    "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore",
]
DTYPES = {c: "Int64" if c.endswith("_time") else "float64" for c in COLS}
NEEDED = {"open_time", "open", "high", "low", "close", "volume"}

# åˆç†æ—¶é—´æˆ³èŒƒå›´ï¼ˆæ¯«ç§’ï¼‰
T_MIN, T_MAX = 1_483_228_800_000, 4_107_926_400_000  # 2017â€‘01â€‘01 ~ 2100â€‘01â€‘01

# CHUNK_SIZE = 100_000  # åˆ†å—å¤„ç†å¤§æ–‡ä»¶ï¼Œæé«˜å†…å­˜æ•ˆç‡


# ----------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# ----------------------------------------------------------------------
def read_kline(path: Path) -> pd.DataFrame:
    """è¯»å–å•ä¸ª ZIP/CSVï¼Œæ— è¡¨å¤´ -> names=COLSï¼Œæ”¯æŒå¤§æ–‡ä»¶ä¼˜åŒ–"""
    try:
        # æ€§èƒ½ä¼˜åŒ–ï¼šæ·»åŠ low_memory=Falseå‡å°‘æ•°æ®ç±»å‹æ¨æ–­çš„å¼€é”€
        return pd.read_csv(
            path,
            header=None,
            names=COLS,
            compression="zip" if path.suffix == ".zip" else None,
            dtype=DTYPES,
            na_values=["", "nan", "null", "None"],
            low_memory=False,
        )
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ–‡ä»¶ {path.name} å¤±è´¥: {e}")
        raise


def analyze_data_quality(df: pd.DataFrame, save_dir: Path) -> dict:
    """åˆ†ææ•°æ®è´¨é‡ï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå’Œå¯è§†åŒ–"""
    save_dir.mkdir(parents=True, exist_ok=True)
    stats = {}
    
    # 1. åŸºæœ¬ç»Ÿè®¡
    stats["æ€»è¡Œæ•°"] = len(df)
    stats["èµ·å§‹æ—¥æœŸ"] = df["timestamp"].min().strftime("%Y-%m-%d")
    stats["ç»“æŸæ—¥æœŸ"] = df["timestamp"].max().strftime("%Y-%m-%d")
    stats["æ—¶é—´åŒºé—´(å¤©)"] = (df["timestamp"].max() - df["timestamp"].min()).days
    stats["æ¯æ—¥Kçº¿æ•°(å¹³å‡)"] = len(df) / stats["æ—¶é—´åŒºé—´(å¤©)"] if stats["æ—¶é—´åŒºé—´(å¤©)"] > 0 else 0
    
    # 2. å¯è§†åŒ–ä»·æ ¼è¶‹åŠ¿
    plt.figure(figsize=(12, 6))
    plt.plot(df["timestamp"], df["close"])
    plt.title("BTC ä»·æ ¼èµ°åŠ¿")
    plt.xlabel("æ—¥æœŸ")
    plt.ylabel("ä»·æ ¼ (USDT)")
    plt.grid(True, alpha=0.3)
    plt.savefig(save_dir / "price_trend.png", dpi=150)
    plt.close()
    
    # 3. å¯è§†åŒ–äº¤æ˜“é‡è¶‹åŠ¿
    plt.figure(figsize=(12, 4))
    plt.plot(df["timestamp"], df["volume"], color="orange")
    plt.title("BTC äº¤æ˜“é‡èµ°åŠ¿")
    plt.xlabel("æ—¥æœŸ")
    plt.ylabel("äº¤æ˜“é‡")
    plt.grid(True, alpha=0.3)
    plt.savefig(save_dir / "volume_trend.png", dpi=150)
    plt.close()
    
    # 4. æ£€æŸ¥è¿ç»­æ€§ - æŸ¥æ‰¾æ—¶é—´ç¼ºå£
    df_sorted = df.sort_values("timestamp")
    df_sorted["next_ts"] = df_sorted["timestamp"].shift(-1)
    df_sorted["gap_hours"] = (df_sorted["next_ts"] - df_sorted["timestamp"]).dt.total_seconds() / 3600
    
    # ç­›é€‰è¶…è¿‡é¢„æœŸé—´éš”çš„ç¼ºå£ï¼ˆå¯¹äº1hæ•°æ®ï¼Œåº”è¯¥æ˜¯1å°æ—¶ï¼‰
    gaps = df_sorted[df_sorted["gap_hours"] > 1.5]
    if len(gaps) > 0:
        stats["æ•°æ®ç¼ºå£æ•°"] = len(gaps)
        stats["æœ€å¤§ç¼ºå£(å°æ—¶)"] = gaps["gap_hours"].max()
        
        # è®°å½•æ˜æ˜¾çš„ç¼ºå£
        with open(save_dir / "data_gaps.csv", "w") as f:
            f.write("start_time,end_time,gap_hours\n")
            for _, row in gaps.iterrows():
                f.write(f"{row['timestamp']},{row['next_ts']},{row['gap_hours']:.1f}\n")
    else:
        stats["æ•°æ®ç¼ºå£æ•°"] = 0
        stats["æœ€å¤§ç¼ºå£(å°æ—¶)"] = 0
    
    # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    with open(save_dir / "data_stats.txt", "w", encoding="utf-8") as f:
        f.write("=== BTC æ•°æ®é›†ç»Ÿè®¡ ===\n\n")
        for k, v in stats.items():
            f.write(f"{k}: {v}\n")
    
    return stats


# ----------------------------------------------------------------------
# ä¸»æµç¨‹
# ----------------------------------------------------------------------
def main() -> None:
    start_time = time.time()
    
    # 1. æ”¶é›†æ–‡ä»¶ (.zip + .csv)
    files = [
        f for f in chain(RAW_DIR.rglob("*.zip"), RAW_DIR.rglob("*.csv"))
        if any(sym in f.name for sym in SYMBOLS)
    ]
    logger.info(f"ğŸ” åŒ¹é…åˆ°æ–‡ä»¶æ•°: {len(files)}")
    if not files:
        logger.error(f"âŒ æœªæ‰¾åˆ° {SYMBOLS} çš„ zip/csv")
        return

    # 2. è¯»å– & åˆå¹¶æ•°æ®
    frames = []
    for f in tqdm(files, desc="è¯»å–æ–‡ä»¶"):
        try:
            df = read_kline(f)
            
            # ç­›é€‰æ‰€éœ€åˆ—
            if not NEEDED.issubset(df.columns):
                logger.warning(f"âš ï¸ æ–‡ä»¶ {f.name} ç¼ºå°‘å¿…è¦åˆ—: {NEEDED}")
                continue
                
            # è¿‡æ»¤å¼‚å¸¸æ—¶é—´æˆ³
            df = df[(df["open_time"] >= T_MIN) & (df["open_time"] <= T_MAX)]
            
            # å–å¿…è¦åˆ—ï¼Œè½¬æ¢æ—¶é—´æˆ³
            df = df[["open_time", "open", "high", "low", "close", "volume"]]
            df["timestamp"] = pd.to_datetime(df["open_time"], unit="ms")
            df = df.drop(columns=["open_time"])
            
            frames.append(df)
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {f.name} å¤±è´¥: {e}")
    
    if not frames:
        logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¸§")
        return
        
    # 3. åˆå¹¶ & æ’åº
    logger.info("ğŸ“Š åˆå¹¶æ•°æ®å¸§...")
    df_all = pd.concat(frames, ignore_index=True)
    
    # åˆ é™¤é‡å¤è®°å½•
    n_before = len(df_all)
    df_all = df_all.drop_duplicates(subset=["timestamp"]).reset_index(drop=True)
    n_after = len(df_all)
    if n_before > n_after:
        logger.info(f"ğŸ§¹ å·²åˆ é™¤ {n_before - n_after} æ¡é‡å¤è®°å½•")
    
    # æŒ‰æ—¶é—´æ’åº
    df_all = df_all.sort_values("timestamp").reset_index(drop=True)
    logger.info(f"âœ… å®Œæˆ! æ€»è¡Œæ•°: {len(df_all)}")
    
    # æ•°æ®åŸºæœ¬æƒ…å†µ
    logger.info(f"ğŸ“… æ•°æ®åŒºé—´: {df_all['timestamp'].min()} ~ {df_all['timestamp'].max()}")
    
    # 4. æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
    logger.info("ğŸ”„ æ·»åŠ æŠ€æœ¯æŒ‡æ ‡...")
    df_with_indicators = add_indicators(df_all)
    logger.info(f"âœ… æ·»åŠ æŒ‡æ ‡åè¡Œæ•°: {len(df_with_indicators)}")
    
    # å¢åŠ ç‰¹å¾å½’ä¸€åŒ–æ­¥éª¤
    logger.info("ğŸ”„ å¯¹æŠ€æœ¯æŒ‡æ ‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†...")
    # æ’é™¤åŸå§‹ä»·æ ¼å’Œäº¤æ˜“é‡åˆ—ï¼Œåªæ ‡å‡†åŒ–è¡ç”ŸæŒ‡æ ‡
    exclude_cols = ["timestamp", "open", "high", "low", "close", "volume"]
    df_normalized = normalize_features(df_with_indicators, method='minmax', exclude_cols=exclude_cols)
    
    # 5. ä¿å­˜å¤„ç†åçš„æ•°æ®
    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    df_normalized.to_parquet(OUT_PATH, compression="snappy")
    logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜: {OUT_PATH}")
    
    # 6. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
    logger.info("ğŸ“ ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")
    stats = analyze_data_quality(df_normalized, STATS_DIR)
    
    # æ‰“å°ä¸€äº›å…³é”®ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
    logger.info("ğŸ“Š ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯:")
    for col in ["return_1d", "rsi", "macd"]:
        if col in df_normalized.columns:
            logger.info(f"  - {col}: å‡å€¼={df_normalized[col].mean():.4f}, æ ‡å‡†å·®={df_normalized[col].std():.4f}")
    
    elapsed = time.time() - start_time
    logger.info(f"ğŸ‰ æ•°æ®å¤„ç†å®Œæˆ! ç”¨æ—¶: {elapsed:.2f}ç§’")
    logger.info(f"ğŸ’¡ æ•°æ®ç»Ÿè®¡: è¡Œæ•°={stats['æ€»è¡Œæ•°']}, æ—¶é—´åŒºé—´={stats['æ—¶é—´åŒºé—´(å¤©)']}å¤©")


if __name__ == "__main__":
    main()