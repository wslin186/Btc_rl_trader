# ================================================================
# 📁  btc_rl_trader / config /config.yaml
# ================================================================

# ------------------------------------------------------------------
# 1️⃣  数据下载
# ------------------------------------------------------------------
download:
  trading_type:  spot
  symbols:      ["BTCUSDT"]
  intervals:    ["1h"]
  start_date:    2017-08-01
  end_date:      2025-04-30
  skip_monthly:  0
  skip_daily:    1
  checksum:      0
  threads:       8
  # 以下两项如果未使用可考虑删除
  frequency:     "@daily"       # 可选
  enable_cron:   false

# ------------------------------------------------------------------
# 2️⃣  数据集区间 / 路径
# ------------------------------------------------------------------
data:
  processed_path: data/processed/btc.parquet

  train_start:  2017-08-01
  train_end:    2023-12-31
  test_start:   2024-01-01
  test_end:     2025-04-30

  train_ratio:  null            # 显式区间时保持 null

# ------------------------------------------------------------------
# 3️⃣  环境参数
# ------------------------------------------------------------------
env:
  window_size: 100         # 保持不变
  init_cash:   100000      # 初始资金
  fee:         0.0004      # 交易所手续费
  slippage:    0.0002      # 滑点
  reward_scale: 5          # 降低奖励尺度，减少模型过激反应 10->5
  action_reward: 0.02      # 提高动作奖励，鼓励明确的决策 0.01->0.02
  trade_penalty: 0.1       # 提高交易惩罚，进一步降低过度交易 0.25->0.1
  sharpe_coef: 1.0         # 增加夏普系数，更强调稳定收益
  position_steps: 1        # 全仓模式
  use_stop_loss: true      # 保持止损功能
  stop_loss_pct: 0.1      # 止损线设置
  position_penalty_ratio: 0.7  # 提高持仓惩罚，增强风险意识
  stop_loss_penalty_ratio: 2.5  # 略微提高止损惩罚
  trade_cooldown: 3       # 增加交易冷却时间，减少频繁交易
  balance_buy_sell: false   # 保持平衡买卖
# ------------------------------------------------------------------
# 4️⃣  PPO 基础超参
# ------------------------------------------------------------------
ppo:
  policy:          MlpPolicy
  n_steps:         2048          # 加倍步数，捕获更长期的市场模式
  batch_size:      512           # 增加批次大小，提高训练稳定性
  learning_rate:   0.0001        # 略微提高学习率，加速收敛
  gamma:           0.998         # 提高未来奖励折扣因子，更注重长期回报 0.995->0.998
  gae_lambda:      0.96          # 微调GAE参数
  ent_coef:        0.001         # 降低熵系数，减少随机探索 0.0005->0.001
  vf_coef:         0.8           # 增加价值函数系数
  clip_range:      0.2
  max_grad_norm:   0.7           # 提高梯度裁剪阈值
  n_epochs:        15            # 增加每批次训练轮数，提高样本利用效率
  total_timesteps: 5000000       # 翻倍训练步数，提高模型性能
  target_kl:       0.015         # 稍微放宽KL散度限制
  verbose:         1
  tensorboard_log: logs/tb
  seed:            42
  device:          auto

# ------------------------------------------------------------------
# 5️⃣  并行训练（SubprocVecEnv）
# ------------------------------------------------------------------
training:
  n_envs:          8           # 增加并行环境数以加速训练
  n_steps:         2048         # 保持与PPO一致
  total_timesteps: 5000000     # 与PPO保持一致
  eval_episodes:   50           # 增加评估回合数，提高评估准确性

# ------------------------------------------------------------------
# 6️⃣  模型与日志路径
# ------------------------------------------------------------------
paths:
  model_dir:  models
  model_name: ppo_btc_latest.zip

tensorboard:
  enable:  true
  log_dir: logs/tb
  host:    127.0.0.1
  port:    6006
